{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49aa5a00-b3e5-4d1e-89ba-c8324fd12ba6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "error",
     "timestamp": 1644693234487,
     "user": {
      "displayName": "Sarim Sikander",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdUSm3KRm0r_9tSILv1kRzQ2JOzhRnOJXfyGjg_o8=s64",
      "userId": "00095935255114898295"
     },
     "user_tz": -300
    },
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "id": "49aa5a00-b3e5-4d1e-89ba-c8324fd12ba6",
    "outputId": "5e18525f-477d-4a4f-dc01-95b79532aeba"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f8b29ad20d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/FINAL FYP/Hatespeech/tokenizer.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/FINAL FYP/Hatespeech/model_load.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/FINAL FYP/Hatespeech/label_encoder.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tokenizer = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Hatespeech/tokenizer.pkl\", \"rb\"))\n",
    "model_load = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Hatespeech/model_load.pkl\", \"rb\"))\n",
    "label_encoder = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Hatespeech/label_encoder.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AAS4_NLs3kHU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AAS4_NLs3kHU",
    "outputId": "13b3ce66-d9ba-4f1b-a0e0-d89ba2366c37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Non-offensive', 'identity_hate', 'neither', 'obscene','offensive', 'sexism\",\n",
       " array([0.   , 0.   , 0.997, 0.003, 0.   , 0.   ], dtype=float32),\n",
       " array(['neither'], dtype=object))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prep_data(text):\n",
    "    tokens = tokenizer(text, max_length=150, truncation=True, \n",
    "                       padding='max_length', \n",
    "                       add_special_tokens=True, \n",
    "                       return_tensors='tf')\n",
    "    tokens = {'input_ids': tf.cast(tokens['input_ids'], tf.float64), 'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)}\n",
    "    headings = '''Non-offensive', 'identity_hate', 'neither', 'obscene','offensive', 'sexism'''\n",
    "    probs = model_load.predict(tokens)[0]\n",
    "    pred = np.argmax(probs)\n",
    "    pred = label_encoder.inverse_transform([pred])\n",
    "    return headings,np.round(probs,3),pred\n",
    "\n",
    "prep_data('hey how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3mLSP15R3gvO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25504,
     "status": "ok",
     "timestamp": 1644693234486,
     "user": {
      "displayName": "Sarim Sikander",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjdUSm3KRm0r_9tSILv1kRzQ2JOzhRnOJXfyGjg_o8=s64",
      "userId": "00095935255114898295"
     },
     "user_tz": -300
    },
    "id": "3mLSP15R3gvO",
    "outputId": "698667e3-ba0a-4a49-acb0-e2011b6cb333"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NgRbqLR73dU5",
   "metadata": {
    "id": "NgRbqLR73dU5"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import nltk\n",
    "# import string\n",
    "\n",
    "# from sklearn.model_selection  import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
    "# from keras.layers.core import Dense, Activation, Dropout\n",
    "# from keras.layers.embeddings import Embedding\n",
    "# from keras.layers import BatchNormalization\n",
    "# from keras.utils import np_utils\n",
    "# from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "# from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "# from keras.preprocessing import sequence, text\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.models import load_model\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import transformers\n",
    "# from transformers import DistilBertTokenizerFast\n",
    "# from transformers import TFAutoModel\n",
    "# import pickle\n",
    "\n",
    "# import re\n",
    "# import string\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# import re\n",
    "\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# import warnings\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Iv5YzWiVLaMR",
   "metadata": {
    "id": "Iv5YzWiVLaMR"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install contextualSpellCheck\n",
    "# !python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1-E2ZBmWhOag",
   "metadata": {
    "id": "1-E2ZBmWhOag"
   },
   "outputs": [],
   "source": [
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Not connected to a GPU')\n",
    "# else:\n",
    "#   print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71513b7d-1758-4f2f-8a19-0936ebf950a2",
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "id": "71513b7d-1758-4f2f-8a19-0936ebf950a2"
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/content/drive/MyDrive/Data/HATESPEECH_UPDATED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ecf8d4-6793-447e-87ce-28acd15277d1",
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "id": "d6ecf8d4-6793-447e-87ce-28acd15277d1"
   },
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G78EYv9wO0p2",
   "metadata": {
    "id": "G78EYv9wO0p2"
   },
   "outputs": [],
   "source": [
    "# data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a4de3-a8ce-46be-8918-f1e95b466393",
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "id": "c69a4de3-a8ce-46be-8918-f1e95b466393"
   },
   "outputs": [],
   "source": [
    "# # X_train = preprocess_text\n",
    "# X_train = data['Text'].values\n",
    "# y_train = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09e115-ca3f-4740-b92a-ef86d2654caf",
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "id": "2f09e115-ca3f-4740-b92a-ef86d2654caf"
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train,test_size=0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bhz3G6GV0cTQ",
   "metadata": {
    "id": "bhz3G6GV0cTQ"
   },
   "outputs": [],
   "source": [
    "# label_encoder = LabelEncoder()\n",
    "# y_train = label_encoder.fit_transform(y_train)\n",
    "# y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NGKJkEkpPLk3",
   "metadata": {
    "id": "NGKJkEkpPLk3"
   },
   "outputs": [],
   "source": [
    "# train = []\n",
    "# for x in X_train:\n",
    "#   train.append(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffcb28a-e705-4a50-b6cb-c4231b2f7161",
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "id": "3ffcb28a-e705-4a50-b6cb-c4231b2f7161"
   },
   "outputs": [],
   "source": [
    "# seq_len = 150\n",
    "# batch_size = 32\n",
    "# num_samples = len(data)\n",
    "# model_name = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f37e01-4223-4aa8-a2db-fb9336273ee4",
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "id": "50f37e01-4223-4aa8-a2db-fb9336273ee4"
   },
   "outputs": [],
   "source": [
    "# tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "# train_tokens = tokenizer(train, max_length=seq_len, \n",
    "#                          truncation=True, padding='max_length', \n",
    "#                          add_special_tokens=True, return_tensors='np')\n",
    "\n",
    "# # y_train = ytrain['target'].values\n",
    "# labels = np.zeros((num_samples, y_train.max() + 1))\n",
    "# labels[np.arange(num_samples), y_train] = 1\n",
    "# # labels = y_train\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((train_tokens['input_ids'], train_tokens['attention_mask'], labels))\n",
    "\n",
    "# def map_func(input_ids, masks, labels):\n",
    "#     return {\n",
    "#         'input_ids': input_ids,\n",
    "#         'attention_mask': masks\n",
    "#     }, labels\n",
    "\n",
    "# dataset = dataset.map(map_func)\n",
    "# dataset = dataset.shuffle(10000).batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "# split = 0.9\n",
    "# size = int((train_tokens['input_ids'].shape[0] // batch_size) * split)\n",
    "\n",
    "# train_ds = dataset.take(size)\n",
    "# val_ds = dataset.skip(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21VH7q__XRVh",
   "metadata": {
    "id": "21VH7q__XRVh"
   },
   "outputs": [],
   "source": [
    "# model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # Two inputs\n",
    "# input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "# mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# # Transformer\n",
    "# embeddings = model.distilbert(input_ids, attention_mask=mask)[0]\n",
    "# #embeddings = model(input_ids, attention_mask=mask)[0]\n",
    "# embeddings = embeddings[:, 0, :]\n",
    "# # Classifier head\n",
    "# x = tf.keras.layers.Dense(512, activation='relu')(embeddings)\n",
    "# # x = tf.keras.layers.Dropout(0.1)(x)\n",
    "# y_ = tf.keras.layers.Dense(len(set(y_train)), activation='softmax', name='outputs')(x)\n",
    "\n",
    "# bert_model = tf.keras.Model(inputs=[input_ids, mask], outputs=y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151cd71-e7bd-4a6c-bb36-600d6f3699b8",
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "id": "2151cd71-e7bd-4a6c-bb36-600d6f3699b8"
   },
   "outputs": [],
   "source": [
    "# # freeze bert layers\n",
    "# # bert_model.layers[2].trainable = False\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(lr=1e-5)\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "# bert_model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "\n",
    "# history = bert_model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=7,\n",
    "#     batch_size=batch_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef91f86-9c04-4f6d-b663-868f0ccec8aa",
   "metadata": {
    "gradient": {
     "editing": false,
     "source_hidden": false
    },
    "id": "5ef91f86-9c04-4f6d-b663-868f0ccec8aa"
   },
   "outputs": [],
   "source": [
    "# def prep_data(text):\n",
    "#     tokens = tokenizer(text, max_length=150, truncation=True, \n",
    "#                        padding='max_length', \n",
    "#                        add_special_tokens=True, \n",
    "#                        return_tensors='tf')\n",
    "#     return {'input_ids': tokens['input_ids'], \n",
    "#             'attention_mask': tokens['attention_mask']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vCvlKJcMf3Lb",
   "metadata": {
    "id": "vCvlKJcMf3Lb"
   },
   "outputs": [],
   "source": [
    "# tokens = prep_data(['Hello, you are too black in face'])\n",
    "# probs = bert_model.predict(tokens)\n",
    "# pred = np.argmax(probs)\n",
    "# pred = label_encoder.inverse_transform([pred])\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gU_lJFw9jwG9",
   "metadata": {
    "id": "gU_lJFw9jwG9"
   },
   "outputs": [],
   "source": [
    "# bert_model.save('/content/drive/MyDrive/MODELS/BERT_HATESPEECH')\n",
    "# tokenizer.save_pretrained('/content/drive/MyDrive/MODELS/BERT_HATESPEECH_TOKENIZER')\n",
    "# pickle.dump(label_encoder, open(\"/content/drive/MyDrive/MODELS/label_encoder_bert_hatespeech.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sHqimVi02JuP",
   "metadata": {
    "id": "sHqimVi02JuP"
   },
   "outputs": [],
   "source": [
    "# pickle.dump(model_load, open(\"/content/drive/MyDrive/FINAL FYP/Hatespeech/model_load.pkl\", \"wb\"))\n",
    "# pickle.dump(tokenizer, open(\"/content/drive/MyDrive/FINAL FYP/Hatespeech/tokenizer.pkl\", \"wb\"))\n",
    "# pickle.dump(label_encoder, open(\"/content/drive/MyDrive/FINAL FYP/Hatespeech/label_encoder.pkl\", \"wb\"))\n",
    "# model_load = load_model('/content/drive/MyDrive/FYP/Hatespeech/BERT_HATESPEECH')\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('/content/drive/MyDrive/FYP/Hatespeech/BERT_HATESPEECH_TOKENIZER')\n",
    "# label_encoder = pickle.load(open(\"/content/drive/MyDrive/FYP/Hatespeech/label_encoder_bert_hatespeech.pkl\", \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Hatespeech_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
