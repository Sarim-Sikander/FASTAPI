{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "10e4390b",
      "metadata": {
        "id": "10e4390b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import spacy\n",
        "import re\n",
        "from itertools import groupby\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "45b1a174",
      "metadata": {
        "id": "45b1a174"
      },
      "outputs": [],
      "source": [
        "model_load = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/model_load.pkl\", \"rb\"))\n",
        "tokenizer = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/tokenizer.pkl\", \"rb\"))\n",
        "label_encoder = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/label_encoder.pkl\", \"rb\"))\n",
        "eng = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/eng.pkl\", \"rb\"))\n",
        "lemma = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/lemma.pkl\", \"rb\"))\n",
        "nlp = spacy.load(\"/content/drive/MyDrive/FINAL FYP/Islamophobia\")\n",
        "stop = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/stop.pkl\", \"rb\"))\n",
        "rel = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/rel.pkl\", \"rb\"))\n",
        "terms = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/terms.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "251e74cd",
      "metadata": {
        "id": "251e74cd"
      },
      "outputs": [],
      "source": [
        "def decontracted(phrase):\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def clean_tweet(tweets):\n",
        "    preprocess_text = []\n",
        "    for tweet in tweets:\n",
        "        tweet = str(tweet).lower()\n",
        "        tweet = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', tweet)\n",
        "        tweet = re.sub('\\$[a-zA-Z0-9]*', ' ', tweet)\n",
        "        tweet = re.sub('\\@[a-zA-Z0-9]*', ' ', tweet)\n",
        "        tweet = re.sub('[^a-zA-Z\\']', ' ', tweet)\n",
        "        tweet = re.sub(r'\\b(?:\\d+|\\w)\\b\\s*', ' ', tweet)\n",
        "        tweet = decontracted(tweet)\n",
        "        tweet=[lemma.lemmatize(x) for x in nltk.word_tokenize(tweet) if x not in stop]\n",
        "        preprocess_text.append(tweet)\n",
        "    return preprocess_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "43e2f0c4",
      "metadata": {
        "id": "43e2f0c4"
      },
      "outputs": [],
      "source": [
        "list_remove = ['islam','muslim','sex','threesome','exmuslims','fuck','fck','ex-muslim','ex muslim','musalman','ex','jerusalem',\n",
        "               'iran','makkah','madina','dick','lesbian','nude','nudity','porn','sexism','racism','religion','prayer','pornhub',\n",
        "               'xxx','xxxxx','xvideos','xnxx','north america','naughty','america','prophet','die','hindu','muslim','hate','kill',\n",
        "               'masjid','women','narrated','allah','muhammad','say','quran','bible','holy','islamophobia','islamophobic','imam',\n",
        "               'children','jihaad','jihad','anti-Muslim','anti-Muslim hate','hate crime','social policy','equality','extremism', \n",
        "               'muslims in greece','hate muslims','end muslims','sex muslims','fuck muslims','muslims in united states','muslims in usa',\n",
        "               'muslims in pakistan','muslims in india','india','indian','mosque attacks','attacks','attack','terror','terrorist','terrorism',\n",
        "               'discrimination','black lifes','ali','namaz','pbuh','worship','god','gods','killer','enemy','palestine','rape']\n",
        "\n",
        "# clean words in the sentences:\n",
        "def search(array, element):\n",
        "    if element in array:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "    \n",
        "def nameMatch(alumniNames, muslimNames):\n",
        "    matches = False\n",
        "    name = alumniNames.split(' ')\n",
        "    lastName=''\n",
        "    if len(name) == 2:\n",
        "        lastName = name[1]\n",
        "        firstName = name[0]\n",
        "        if search(muslimNames, firstName) or search(muslimNames,lastName):\n",
        "            matches = True\n",
        "        else:\n",
        "            matches=False\n",
        "    else:\n",
        "        firstName = name[0]\n",
        "        if search(muslimNames, firstName):\n",
        "            matches = True\n",
        "        else:\n",
        "            matches=False\n",
        "    return matches\n",
        "    \n",
        "def diff_pos_neg_check(pat,arr):\n",
        "    result_final=False\n",
        "    result_binary = False\n",
        "    result_kmp = False\n",
        "    string=''\n",
        "    result = search(arr,pat)\n",
        "\n",
        "    if result != False:\n",
        "        result_binary = True\n",
        "    else:\n",
        "        pass;\n",
        "#         for word in arr:\n",
        "#             result = patternSearching(pat, word)\n",
        "#             if result == None:\n",
        "#                 result_kmp = False\n",
        "#             else:\n",
        "#                 result_kmp = True\n",
        "    if result_binary or result_kmp:\n",
        "        result_final=True\n",
        "    else:\n",
        "        result_final=False\n",
        "    if result_final:\n",
        "        string='found'\n",
        "        return string\n",
        "    else:\n",
        "        string='not found'\n",
        "        return string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "8d2290fa",
      "metadata": {
        "id": "8d2290fa"
      },
      "outputs": [],
      "source": [
        "def test_re(s):\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    return regex.sub(' ', s)\n",
        "def sim_check(text):\n",
        "    abusive=False\n",
        "    religious=False\n",
        "    ban=False\n",
        "    clean=False\n",
        "    check = False\n",
        "    result_=False\n",
        "    text_lemma = ''\n",
        "    result_rel=[]\n",
        "    result_abusive=[]\n",
        "    text_clean = []\n",
        "    \n",
        "    text = [text]\n",
        "    for t in text:\n",
        "        doc = nlp(t)\n",
        "    for tokens in doc:\n",
        "        text_lemma = text_lemma+' '+tokens.lemma_\n",
        "    text_lemma = text_lemma.strip()\n",
        "    text_lemma = test_re(text_lemma)\n",
        "    splitted_text = text_lemma.lower().split()\n",
        "    text_clean.append([word for word in splitted_text if word not in stop])\n",
        "    text_clean = [x for sublist in text_clean for x in sublist]\n",
        "    for word in range(len(text_clean)):\n",
        "        rep_rem = \"\".join(c for c, _ in groupby(text_clean[word]))\n",
        "        if eng.check(rep_rem) or (rep_rem in terms) or (rep_rem in rel):\n",
        "            text_clean[word] = rep_rem\n",
        "        else:\n",
        "            pass;\n",
        "        \n",
        "    text = \" \".join(text_clean)\n",
        "    for s in text.split():\n",
        "        try:\n",
        "            result_abusive.append(diff_pos_neg_check(s, terms))\n",
        "            result_rel.append(diff_pos_neg_check(s, rel))\n",
        "        except:\n",
        "            print('Word length is less')\n",
        "    \n",
        "    if ('found' in result_abusive) and ('found' in result_rel):\n",
        "        ban=True\n",
        "        abusive=False\n",
        "        religious=False\n",
        "    else:\n",
        "        if 'found' in result_rel:\n",
        "            religious=True\n",
        "        else:\n",
        "            clean=True\n",
        "        if 'found' in result_abusive:\n",
        "            abusive=True\n",
        "        else:\n",
        "            clean=True\n",
        "\n",
        "    if ban:\n",
        "        return 'Strong Islamophobia'\n",
        "    else:\n",
        "        if abusive and religious:\n",
        "            result_=True\n",
        "        else:\n",
        "            if religious:\n",
        "                check=True\n",
        "            else:\n",
        "                if abusive:\n",
        "                    return 'Weak Islamophobia'\n",
        "                else:\n",
        "                    return 'No-Islamophobia'    \n",
        "    if check:\n",
        "        doc = nlp(text)\n",
        "        for token in doc:\n",
        "            if (token.lemma_ in rel and token.dep_ in ('nsubj','dobj')):\n",
        "                try:\n",
        "                    if doc[token.i+2].tag_ in ('JJ','VBG','VBZ','VB','VBP','RB','VBD') and doc[token.i+2].text in terms:\n",
        "                        result_=True\n",
        "                except:\n",
        "                    try:\n",
        "                        if doc[token.i+1].dep_ == 'neg' or doc[token.i+2].dep_ in 'neg':\n",
        "                            if doc[token.i+1].lemma_ == 'not' or doc[token.i+2].lemma_ == 'not':\n",
        "                                result_=True\n",
        "                    except:\n",
        "                        if doc[token.i-1].tag_ in ('VBG','VBD','VB','VBP','VBZ') and doc[token.i-2].dep_ in ('prep','aux') and (doc[token.i-3].tag_ in ('JJ','RB','NNS') or doc[token.i-2].tag_ in ('JJ','RB','NNS')):\n",
        "                            result_=True\n",
        "                        elif doc[token.i-1].tag_ in ('DT') and doc[token.i-1].dep_ in ('dobj','det') and doc[token.i-2].tag_ in ('VBG','VBD'):\n",
        "                            result_=True\n",
        "                        elif (doc[token.i-1].tag_ in ('VBG','VBD','VB','VBP','VBZ') or doc[token.i-2].dep_ in ('prep','aux')) and (doc[token.i-3].tag_ in ('JJ','RB') or doc[token.i-2].tag_ in ('JJ','RB')):\n",
        "                            result_=True\n",
        "        if result_:\n",
        "            return 'Strong Islamophobia'\n",
        "        else:\n",
        "            return 'No-Islamophobia'\n",
        "    else:\n",
        "        pass;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "e52d1251",
      "metadata": {
        "id": "e52d1251"
      },
      "outputs": [],
      "source": [
        "def prep_data(text):\n",
        "    tokens = tokenizer(text, max_length=150, truncation=True, \n",
        "                      padding='max_length', \n",
        "                      add_special_tokens=True, \n",
        "                      return_tensors='tf')\n",
        "    tokens = {'input_ids': tf.cast(tokens['input_ids'], tf.float64), 'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)}\n",
        "    probs = model_load.predict(tokens)[0]\n",
        "    pred = np.argmax(probs)\n",
        "    pred = label_encoder.inverse_transform([pred])\n",
        "    return np.round(probs,3),pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "c8323c85",
      "metadata": {
        "id": "c8323c85"
      },
      "outputs": [],
      "source": [
        "def self_learning(U,rulesystem):\n",
        "    model_check_s = False\n",
        "    model_check_w = False\n",
        "    model_check_n = False\n",
        "    check = ''\n",
        "    rule_answer = ''\n",
        "    model_answer = ''\n",
        "    labels = ['Strong Islamophobia', 'Weak Islamophobia', 'No-Islamophobia']\n",
        "    for i in range(len(U)):\n",
        "        try:\n",
        "            sentence = clean_tweet([U[i]])\n",
        "            for t in sentence:\n",
        "                text = t\n",
        "            sent = ' '.join(text)\n",
        "            o = prep_data(sent)\n",
        "            rule_answer = rulesystem(sent)\n",
        "            model_answer = o[1]\n",
        "            if rule_answer == model_answer:\n",
        "                return model_answer\n",
        "            elif rule_answer == labels[0] and model_answer in ('No-Islamophobia','Weak Islamophobia'):\n",
        "                model_check_s = True\n",
        "            else:\n",
        "                if o[0][2]>0.05 and o[0][2]<0.50 and o[0][1] < o[0][2]:\n",
        "                    model_check_w=True\n",
        "                elif o[0][2]<0.010 and o[0][1]<0.010:\n",
        "                    model_check_n=True\n",
        "        except:\n",
        "            pass;\n",
        "        if model_check_w:\n",
        "            check = 'Weak Islamophobia'\n",
        "        elif model_check_s:\n",
        "            check = 'Strong Islamophobia'\n",
        "        elif model_check_n:\n",
        "            check = 'No-Islamophobia'\n",
        "        for label in labels:\n",
        "            if label in {check,rule_answer,model_answer[0]}:\n",
        "              return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "XqW4iiXLrCLB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XqW4iiXLrCLB",
        "outputId": "dfe324f5-756a-461e-8193-56398afb45df"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Weak Islamophobia'"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "self_learning(['i love islam'],rulesystem=sim_check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3483f06",
      "metadata": {
        "id": "c3483f06"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# pickle.dump(stop_words, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/stop_words.pkl','wb'))\n",
        "# pickle.dump(eng, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/eng.pkl','wb'))\n",
        "# pickle.dump(lemma, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/lemma.pkl','wb'))\n",
        "# pickle.dump(nlp, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/nlp.pkl','wb'))\n",
        "# rel = pd.read_csv('/content/drive/MyDrive/FYP/islamophobia complete/words_rel.csv')\n",
        "# terms = pd.read_csv('/content/drive/MyDrive/FYP/islamophobia complete/words.csv')\n",
        "# stop = stopwords.words('english')\n",
        "# terms=set(terms['0'])\n",
        "# rel = set(rel['0'])\n",
        "# terms.remove(np.nan)\n",
        "# print(rel)\n",
        "# pickle.dump(stop, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/stop.pkl','wb'))\n",
        "# pickle.dump(rel, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/rel.pkl','wb'))\n",
        "# pickle.dump(terms, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/terms.pkl','wb'))\n",
        "# !apt install -qq enchant\n",
        "# !pip install pyenchant\n",
        "# !pip install transformers\n",
        "# !python -m spacy download en_core_web_lg\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ywBJ7SOhovWC",
      "metadata": {
        "id": "ywBJ7SOhovWC"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(decontracted, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/decontracted.pkl','wb'))\n",
        "# pickle.dump(clean_tweet, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/clean_tweet.pkl','wb'))\n",
        "# pickle.dump(list_remove, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/list_remove.pkl','wb'))\n",
        "# pickle.dump(nameMatch, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/nameMatch.pkl','wb'))\n",
        "# pickle.dump(diff_pos_neg_check, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/diff_pos_neg_check.pkl','wb'))\n",
        "# pickle.dump(test_re, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/test_re.pkl','wb'))\n",
        "# pickle.dump(sim_check, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/sim_check.pkl','wb'))\n",
        "# pickle.dump(prep_data, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/prep_data.pkl','wb'))\n",
        "# pickle.dump(self_learning, open('/content/drive/MyDrive/FINAL FYP/Islamophobia/self_learning.pkl','wb'))\n",
        "\n",
        "# decontracted = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/decontracted.pkl\", \"rb\"))\n",
        "# clean_tweet = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/clean_tweet.pkl\", \"rb\"))\n",
        "# list_remove = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/list_remove.pkl\", \"rb\"))\n",
        "# nameMatch = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/nameMatch.pkl\", \"rb\"))\n",
        "# diff_pos_neg_check = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/diff_pos_neg_check.pkl\", \"rb\"))\n",
        "# test_re = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/test_re.pkl\", \"rb\"))\n",
        "# sim_check = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/sim_check.pkl\", \"rb\"))\n",
        "# prep_data = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/prep_data.pkl\", \"rb\"))\n",
        "# self_learning = pickle.load(open(\"/content/drive/MyDrive/FINAL FYP/Islamophobia/self_learning.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bYTmccc65Al8",
      "metadata": {
        "id": "bYTmccc65Al8"
      },
      "outputs": [],
      "source": [
        "# pickle.dump(model_load,open('/content/drive/MyDrive/FINAL FYP/Islamophobia/model_load.pkl','wb'))\n",
        "# pickle.dump(tokenizer,open('/content/drive/MyDrive/FINAL FYP/Islamophobia/tokenizer.pkl','wb'))\n",
        "# pickle.dump(label_encoder,open('/content/drive/MyDrive/FINAL FYP/Islamophobia/label_encoder.pkl','wb'))\n",
        "# model_load = load_model('/content/drive/MyDrive/MODELS/BERT_ISLAMOPHOBIA/')\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained('/content/drive/MyDrive/MODELS/BERT_ISLAMOPHOBIA_TOKENIZER/')\n",
        "# label_encoder = pickle.load(open(\"/content/drive/MyDrive/MODELS/label_encoder_bert_islamophobia.pkl\", \"rb\"))\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import nltk\n",
        "# import string\n",
        "# import nltk\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# from nltk.corpus import stopwords\n",
        "# from sklearn.model_selection  import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder\n",
        "# from keras.models import load_model\n",
        "# import spacy \n",
        "# import re\n",
        "# from itertools import groupby\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# import enchant\n",
        "# import pickle\n",
        "# from transformers import DistilBertTokenizer\n",
        "# import tensorflow as tf\n",
        "# eng=enchant.Dict('en_US')\n",
        "# lemma = WordNetLemmatizer()\n",
        "# nlp = spacy.load(\"/content/drive/MyDrive/FINAL FYP/Islamophobia\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Islamophobia.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
